{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae77620",
   "metadata": {},
   "source": [
    "# Reconhecimento de Padrões (TIP8311) - Trabalho 3\n",
    "\n",
    "\n",
    "**Professor:** Guilherme de Alencar Barreto  \n",
    "\n",
    "<img src=\"https://loop.frontiersin.org/images/profile/243428/203\" alt=\"Foto do Professor\" width=\"150\"/>\n",
    "\n",
    "\n",
    "**Aluno:** Luis Felipe Carneiro de Souza    **Matrícula:** 535049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80be277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.spatial.distance import minkowski\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5e3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"wall+following+robot+navigation+data/sensor_readings_24.data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70be56",
   "metadata": {},
   "source": [
    "## Questão 01\n",
    "\n",
    "Para este trabalho computacional, considere o conjunto de dados disponível no site abaixo.\n",
    "\n",
    "Usando o conjunto original para 24 sensores de ultrassom. Pede-se:\n",
    "\n",
    "1.1. Identificar para o problema em questão o número de classes, o númeor de instâncias/exemplos de cada classe e a dimensão do vetor de atributos.\n",
    "\n",
    "1.2. Verificar se as matrizes de covariância das classes são invertíveis ou não.\n",
    "\n",
    "1.3. Implementar e avaliar os seguintes classificadors: (1) classificador quadrático gaussiano (CQG) e (2) classificador de distância mímina ao protótipo¹ (DMP). Preencher a tabela abaixo após Nr = 100 Rodadas de treinamento/teste. Comente os resultados obtidos.\n",
    "\n",
    "\n",
    "**Classificador ** | Média Desvio Padrão |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76da6963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5456, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt(fname=data_path, delimiter=\",\", dtype=str, encoding=\"utf-8\")\n",
    "\n",
    "X = data[:, :-1].astype(float)  # todas menos a última coluna convertidas para float\n",
    "y = data[:, -1]                 # última coluna como string\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# X, y\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc07d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQG:\n",
    "    def __init__(self, reg=1e-8):\n",
    "        self.C = None         # classes\n",
    "        self.mcov = None      # dict: classe -> matriz de covariância (p x p)\n",
    "        self.means = None     # dict: classe -> vetor média (p x 1)\n",
    "        self.priors = None    # dict: classe -> prior f(ωi)\n",
    "        self.reg = reg        # regularização para estabilidade numérica\n",
    "\n",
    "    def _mcov(self, X_i):\n",
    "        # X_i: shape (p, N) -> p = nº features, N = nº amostras\n",
    "        p, N = X_i.shape\n",
    "        m = X_i.mean(axis=1).reshape(-1,1)\n",
    "        R = (X_i @ X_i.T) / N\n",
    "        C = R - (m @ m.T)\n",
    "        return C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Treina o modelo.\n",
    "        X: (n_samples, n_features)\n",
    "        y: (n_samples, 1) ou (n_samples,)\n",
    "        \"\"\"\n",
    "        y_flat = y.ravel()\n",
    "        self.C = np.sort(np.unique(y_flat))\n",
    "        self.mcov = {}\n",
    "        self.means = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        n = X.shape[0]\n",
    "        for c in self.C:\n",
    "            idx = np.where(y_flat == c)[0]\n",
    "            Xc = X[idx]                 # shape (Nc, p)\n",
    "            Nc = Xc.shape[0]\n",
    "            if Nc == 0:\n",
    "                raise ValueError(f\"Classe {c} sem amostras.\")\n",
    "\n",
    "            # média como vetor coluna (p,1)\n",
    "            mean_c = Xc.mean(axis=0).reshape(-1,1)\n",
    "\n",
    "            # covariância usando _mcov (espera p x N)\n",
    "            cov_c = self._mcov(Xc.T)\n",
    "\n",
    "            # regularização na diagonal para evitar singularidade\n",
    "            cov_c = cov_c + self.reg * np.eye(cov_c.shape[0])\n",
    "\n",
    "            prior_c = Nc / n\n",
    "\n",
    "            self.means[c] = mean_c\n",
    "            self.mcov[c] = cov_c\n",
    "            self.priors[c] = prior_c\n",
    "\n",
    "    def _Qi(self, x_col, mean_col, inv_cov):\n",
    "        \"\"\"\n",
    "        Calcula Q_i(x) = (x - m_i)^T C_i^{-1} (x - m_i)\n",
    "        x_col, mean_col: (p,1)\n",
    "        inv_cov: (p,p)\n",
    "        retorna escalar\n",
    "        \"\"\"\n",
    "        diff = x_col - mean_col\n",
    "        return diff.T @ inv_cov @ diff\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Prediz rótulos para X (n_samples, n_features).\n",
    "        Retorna array com rótulos (mesmo tipo de self.C).\n",
    "        \"\"\"\n",
    "        if self.C is None:\n",
    "            raise ValueError(\"Modelo não treinado. Chame fit(X, y) primeiro.\")\n",
    "\n",
    "        n, p = X.shape\n",
    "        preds = np.empty(n, dtype=self.C.dtype)\n",
    "\n",
    "        # pré-computar inversas e log-determinantes por classe\n",
    "        invs = {}\n",
    "        logdets = {}\n",
    "        for c in self.C:\n",
    "            cov = self.mcov[c]\n",
    "            # usar slogdet para estabilidade\n",
    "            sign, logdet = np.linalg.slogdet(cov)\n",
    "            if sign <= 0:\n",
    "                # fallback (deveria ser raro por causa da regularização)\n",
    "                logdet = np.log(np.linalg.det(cov) + 1e-20)\n",
    "            invs[c] = np.linalg.inv(cov)\n",
    "            logdets[c] = logdet\n",
    "\n",
    "        for i in range(n):\n",
    "            x_col = X[i].reshape(-1,1)\n",
    "            best_c = None\n",
    "            best_g = np.inf  # queremos o menor g_i^*(x)\n",
    "            for c in self.C:\n",
    "                inv_cov = invs[c]\n",
    "                Qi = self._Qi(x_col, self.means[c], inv_cov)\n",
    "                # g*_i(x) = Qi(x) + ln(|Ci|) - 2 ln(f(ωi))\n",
    "                prior = self.priors[c]\n",
    "                # evitar log(0)\n",
    "                if prior <= 0:\n",
    "                    raise ValueError(f\"Prior da classe {c} é zero.\")\n",
    "                g = Qi + logdets[c] - 2.0 * np.log(prior)\n",
    "                if g < best_g:\n",
    "                    best_g = g\n",
    "                    best_c = c\n",
    "            preds[i] = best_c\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46e711e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    \"\"\"K-means sequencial (online) com alpha = 1 / count_i(t) e kmeans++.\"\"\"\n",
    "\n",
    "    def __init__(self, k=3, max_epochs=100, tol=1e-4, random_state=None,\n",
    "                 init='random', handle_empty='reinit'):\n",
    "        self.k = int(k)\n",
    "        self.max_epochs = int(max_epochs)\n",
    "        self.tol = float(tol)\n",
    "        self.random_state = random_state\n",
    "        self.init = init\n",
    "        self.handle_empty = handle_empty\n",
    "\n",
    "        self.centroids = None\n",
    "        self.counts = None\n",
    "        self.inertia_ = None\n",
    "        self.n_iter_ = 0\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.asarray(X, dtype=float)\n",
    "        else:\n",
    "            X = X.astype(float, copy=False)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X deve ser 2D com shape (n_samples, n_features).\")\n",
    "        if np.isnan(X).any():\n",
    "            raise ValueError(\"X contém NaNs. Trate-os antes de ajustar.\")\n",
    "        return X\n",
    "\n",
    "    def _init_centroids(self, X):\n",
    "        n, _ = X.shape\n",
    "        if self.k > n:\n",
    "            raise ValueError(\"k não pode ser maior que o número de amostras.\")\n",
    "\n",
    "        if self.init == 'random':\n",
    "            idx = self.rng.choice(n, size=self.k, replace=False)\n",
    "            self.centroids = X[idx].astype(float).copy()\n",
    "        elif self.init == 'kmeans++':\n",
    "            # kmeans++ initialization\n",
    "            centroids = np.empty((self.k, X.shape[1]), dtype=float)\n",
    "            # 1) choose first centroid uniformly at random\n",
    "            first_idx = int(self.rng.integers(0, n))\n",
    "            centroids[0] = X[first_idx]\n",
    "            # 2) choose remaining centroids\n",
    "            # distances squared to nearest chosen centroid\n",
    "            closest_dist_sq = np.sum((X - centroids[0])**2, axis=1)\n",
    "            for c in range(1, self.k):\n",
    "                # probability proportional to distance squared\n",
    "                total = closest_dist_sq.sum()\n",
    "                if total == 0.0:\n",
    "                    # all points identical to chosen centroids; pick random remaining\n",
    "                    next_idx = int(self.rng.integers(0, n))\n",
    "                else:\n",
    "                    probs = closest_dist_sq / total\n",
    "                    next_idx = int(self.rng.choice(n, p=probs))\n",
    "                centroids[c] = X[next_idx]\n",
    "                # update closest distances squared\n",
    "                dist_to_new = np.sum((X - centroids[c])**2, axis=1)\n",
    "                closest_dist_sq = np.minimum(closest_dist_sq, dist_to_new)\n",
    "            self.centroids = centroids\n",
    "        else:\n",
    "            # fallback para random\n",
    "            idx = self.rng.choice(n, size=self.k, replace=False)\n",
    "            self.centroids = X[idx].astype(float).copy()\n",
    "\n",
    "        self.counts = np.zeros(self.k, dtype=int)\n",
    "\n",
    "    def _handle_empty_clusters(self, X, labels):\n",
    "        for i in range(self.k):\n",
    "            if np.sum(labels == i) == 0:\n",
    "                if self.handle_empty == 'reinit':\n",
    "                    self.centroids[i] = X[self.rng.integers(0, X.shape[0])]\n",
    "                    self.counts[i] = 0\n",
    "                elif self.handle_empty == 'farthest':\n",
    "                    dists = np.linalg.norm(X[:, None] - self.centroids[None, :], axis=2)\n",
    "                    far_idx = np.argmax(np.min(dists, axis=1))\n",
    "                    self.centroids[i] = X[far_idx]\n",
    "                    self.counts[i] = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.centroids is None:\n",
    "            raise RuntimeError(\"Modelo não treinado. Chame fit(X) primeiro.\")\n",
    "        X = self._check_X(X)\n",
    "        dists = np.linalg.norm(X[:, None, :] - self.centroids[None, :, :], axis=2)\n",
    "        return np.argmin(dists, axis=1)\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = self._check_X(X)\n",
    "        n, _ = X.shape\n",
    "        self._init_centroids(X)\n",
    "        prev_centroids = self.centroids.copy()\n",
    "\n",
    "        for epoch in range(1, self.max_epochs + 1):\n",
    "            perm = self.rng.permutation(n)\n",
    "            X_shuffled = X[perm]\n",
    "\n",
    "            for x in X_shuffled:\n",
    "                dists = np.linalg.norm(self.centroids - x, axis=1)\n",
    "                i_star = int(np.argmin(dists))\n",
    "                self.counts[i_star] += 1\n",
    "                alpha = 1.0 / self.counts[i_star]\n",
    "                self.centroids[i_star] = (1 - alpha) * self.centroids[i_star] + alpha * x\n",
    "\n",
    "            labels = self.predict(X)\n",
    "            self._handle_empty_clusters(X, labels)\n",
    "\n",
    "            shift = np.linalg.norm(self.centroids - prev_centroids)\n",
    "            prev_centroids = self.centroids.copy()\n",
    "            self.n_iter_ = epoch\n",
    "\n",
    "            dists = np.linalg.norm(X - self.centroids[labels], axis=1)\n",
    "            self.inertia_ = float(np.sum(dists ** 2))\n",
    "\n",
    "            if shift < self.tol:\n",
    "                break\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- Funções manuais de índices e utilitários ----------\n",
    "def _pairwise_distances(X, Y=None):\n",
    "    if Y is None:\n",
    "        Y = X\n",
    "    XX = np.sum(X * X, axis=1)[:, None]\n",
    "    YY = np.sum(Y * Y, axis=1)[None, :]\n",
    "    D2 = XX + YY - 2 * (X @ Y.T)\n",
    "    D2 = np.maximum(D2, 0.0)\n",
    "    return np.sqrt(D2)\n",
    "\n",
    "def silhouette_score_manual(X, labels):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    labels = np.asarray(labels)\n",
    "    unique = np.unique(labels)\n",
    "    n_clusters = unique.size\n",
    "    n_samples = X.shape[0]\n",
    "    if n_clusters < 2 or n_samples <= 1:\n",
    "        return -1.0\n",
    "    D = _pairwise_distances(X)\n",
    "    sil_vals = np.zeros(n_samples, dtype=float)\n",
    "    for idx in range(n_samples):\n",
    "        lab = labels[idx]\n",
    "        same_idx = np.where(labels == lab)[0]\n",
    "        if same_idx.size <= 1:\n",
    "            a = 0.0\n",
    "        else:\n",
    "            a = (np.sum(D[idx, same_idx]) - 0.0) / (same_idx.size - 1)\n",
    "        b_vals = []\n",
    "        for other_lab in unique:\n",
    "            if other_lab == lab:\n",
    "                continue\n",
    "            other_idx = np.where(labels == other_lab)[0]\n",
    "            if other_idx.size == 0:\n",
    "                continue\n",
    "            b_vals.append(np.mean(D[idx, other_idx]))\n",
    "        b = np.min(b_vals) if len(b_vals) > 0 else 0.0\n",
    "        denom = max(a, b)\n",
    "        sil_vals[idx] = 0.0 if denom == 0.0 else (b - a) / denom\n",
    "    return float(np.mean(sil_vals))\n",
    "\n",
    "def calinski_harabasz_score_manual(X, labels):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    labels = np.asarray(labels)\n",
    "    n_samples, _ = X.shape\n",
    "    unique = np.unique(labels)\n",
    "    k = unique.size\n",
    "    if k < 2 or n_samples <= k:\n",
    "        return -np.inf\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    B = 0.0\n",
    "    W = 0.0\n",
    "    for lab in unique:\n",
    "        Xi = X[labels == lab]\n",
    "        ni = Xi.shape[0]\n",
    "        if ni == 0:\n",
    "            continue\n",
    "        mean_i = np.mean(Xi, axis=0)\n",
    "        B += ni * np.sum((mean_i - overall_mean) ** 2)\n",
    "        W += np.sum((Xi - mean_i) ** 2)\n",
    "    if W == 0.0:\n",
    "        return np.inf\n",
    "    score = (B * (n_samples - k)) / (W * (k - 1))\n",
    "    return float(score)\n",
    "\n",
    "def davies_bouldin_score_manual(X, labels):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    labels = np.asarray(labels)\n",
    "    unique = np.unique(labels)\n",
    "    k = unique.size\n",
    "    if k < 2:\n",
    "        return np.inf\n",
    "    centroids = []\n",
    "    s = []\n",
    "    for lab in unique:\n",
    "        Xi = X[labels == lab]\n",
    "        if Xi.shape[0] == 0:\n",
    "            centroids.append(np.zeros(X.shape[1], dtype=float))\n",
    "            s.append(0.0)\n",
    "            continue\n",
    "        mean_i = np.mean(Xi, axis=0)\n",
    "        centroids.append(mean_i)\n",
    "        dists = np.linalg.norm(Xi - mean_i, axis=1)\n",
    "        s.append(np.mean(dists) if dists.size > 0 else 0.0)\n",
    "    centroids = np.vstack(centroids)\n",
    "    s = np.array(s, dtype=float)\n",
    "    C = _pairwise_distances(centroids)\n",
    "    R = np.full((k, k), -np.inf, dtype=float)\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            if i == j:\n",
    "                continue\n",
    "            denom = C[i, j]\n",
    "            R[i, j] = np.inf if denom == 0.0 else (s[i] + s[j]) / denom\n",
    "    R_i = np.max(R, axis=1)\n",
    "    return float(np.mean(R_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7fdbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMP:\n",
    "    \"\"\"\n",
    "    Classificador DMP com múltiplos protótipos por classe.\n",
    "    Usa Kmeans externo (deve existir no escopo) e índices manuais.\n",
    "    \"\"\"\n",
    "    def __init__(self, k_max=5, Nr=5, kmeans_params=None, validation_indices=None):\n",
    "        self.k_max = int(k_max)\n",
    "        self.Nr = int(Nr)\n",
    "        self.kmeans_params = {} if kmeans_params is None else dict(kmeans_params)\n",
    "        self.validation_indices = ['silhouette', 'calinski', 'davies'] if validation_indices is None else validation_indices\n",
    "\n",
    "        self.class_prototypes_ = {}\n",
    "        self.class_Kopt_ = {}\n",
    "        self.prototype_labels_ = None\n",
    "        self.prototypes_ = None\n",
    "\n",
    "    def _validate_X(self, X):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.asarray(X, dtype=float)\n",
    "        else:\n",
    "            X = X.astype(float, copy=False)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X deve ser 2D com shape (n_samples, n_features).\")\n",
    "        if np.isnan(X).any():\n",
    "            raise ValueError(\"X contém NaNs. Trate-os antes de ajustar.\")\n",
    "        return X\n",
    "\n",
    "    def _validate_y(self, y, n_samples):\n",
    "        y = np.asarray(y).ravel()\n",
    "        if y.shape[0] != n_samples:\n",
    "            raise ValueError(\"y deve ter o mesmo número de amostras que X.\")\n",
    "        return y\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = self._validate_X(X)\n",
    "        y = self._validate_y(y, X.shape[0])\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        for cls in classes:\n",
    "            mask = (y == cls)\n",
    "            Xc = X[mask]\n",
    "            nc = Xc.shape[0]\n",
    "            if nc == 0:\n",
    "                continue\n",
    "            k_max_local = min(self.k_max, nc)\n",
    "            best_centroids_for_k = {}\n",
    "            best_ssd_for_k = {}\n",
    "\n",
    "            for k in range(1, k_max_local + 1):\n",
    "                best_ssd = np.inf\n",
    "                best_centroids = None\n",
    "                for r in range(self.Nr):\n",
    "                    params = dict(self.kmeans_params)\n",
    "                    params.update({'k': k})\n",
    "                    km = Kmeans(**params)   # Kmeans deve estar definido\n",
    "                    km.fit(Xc)\n",
    "                    ssd = km.inertia_ if hasattr(km, 'inertia_') else np.sum((Xc - km.centroids[km.predict(Xc)])**2)\n",
    "                    if ssd < best_ssd:\n",
    "                        best_ssd = ssd\n",
    "                        best_centroids = km.centroids.copy()\n",
    "                best_centroids_for_k[k] = best_centroids\n",
    "                best_ssd_for_k[k] = best_ssd\n",
    "\n",
    "            index_scores = {'silhouette': {}, 'calinski': {}, 'davies': {}}\n",
    "            for k, centroids in best_centroids_for_k.items():\n",
    "                dists = np.linalg.norm(Xc[:, None, :] - centroids[None, :, :], axis=2)\n",
    "                labels_k = np.argmin(dists, axis=1)\n",
    "                index_scores['silhouette'][k] = silhouette_score_manual(Xc, labels_k)\n",
    "                index_scores['calinski'][k] = calinski_harabasz_score_manual(Xc, labels_k)\n",
    "                index_scores['davies'][k] = davies_bouldin_score_manual(Xc, labels_k)\n",
    "\n",
    "            suggested_K = []\n",
    "            if 'silhouette' in self.validation_indices:\n",
    "                best_k_sil = max(index_scores['silhouette'].items(), key=lambda kv: kv[1])[0]\n",
    "                suggested_K.append(best_k_sil)\n",
    "            if 'calinski' in self.validation_indices:\n",
    "                best_k_cal = max(index_scores['calinski'].items(), key=lambda kv: kv[1])[0]\n",
    "                suggested_K.append(best_k_cal)\n",
    "            if 'davies' in self.validation_indices:\n",
    "                best_k_dav = min(index_scores['davies'].items(), key=lambda kv: kv[1])[0]\n",
    "                suggested_K.append(best_k_dav)\n",
    "\n",
    "            counts = Counter(suggested_K)\n",
    "            most_common = counts.most_common()\n",
    "            if len(most_common) == 0:\n",
    "                Kopt = 1\n",
    "            else:\n",
    "                top_count = most_common[0][1]\n",
    "                candidates = [k for k, cnt in most_common if cnt == top_count]\n",
    "                Kopt = min(candidates)\n",
    "\n",
    "            self.class_prototypes_[cls] = best_centroids_for_k[Kopt]\n",
    "            self.class_Kopt_[cls] = Kopt\n",
    "\n",
    "        prototypes_list = []\n",
    "        proto_labels = []\n",
    "        for cls in classes:\n",
    "            cent = self.class_prototypes_.get(cls)\n",
    "            if cent is None:\n",
    "                continue\n",
    "            prototypes_list.append(cent)\n",
    "            proto_labels.extend([cls] * cent.shape[0])\n",
    "\n",
    "        if len(prototypes_list) == 0:\n",
    "            raise RuntimeError(\"Nenhum protótipo foi gerado. Verifique os dados e parâmetros.\")\n",
    "\n",
    "        self.prototypes_ = np.vstack(prototypes_list)\n",
    "        self.prototype_labels_ = np.array(proto_labels)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.prototypes_ is None:\n",
    "            raise RuntimeError(\"Modelo não treinado. Chame fit(X, y) primeiro.\")\n",
    "        X = self._validate_X(X)\n",
    "        dists = np.linalg.norm(X[:, None, :] - self.prototypes_[None, :, :], axis=2)\n",
    "        idx = np.argmin(dists, axis=1)\n",
    "        return self.prototype_labels_[idx].reshape(-1, 1)\n",
    "\n",
    "    def predict_proba_distance(self, X):\n",
    "        if self.prototypes_ is None:\n",
    "            raise RuntimeError(\"Modelo não treinado. Chame fit(X, y) primeiro.\")\n",
    "        X = self._validate_X(X)\n",
    "        dists = np.linalg.norm(X[:, None, :] - self.prototypes_[None, :, :], axis=2)\n",
    "        idx = np.argmin(dists, axis=1)\n",
    "        min_dists = dists[np.arange(dists.shape[0]), idx]\n",
    "        return min_dists, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3da1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"\n",
    "    PCA manual com duas abordagens:\n",
    "      - method='eig' : autovalores/autovetores da matriz de covariancia\n",
    "      - method='svd' : SVD da matriz de dados centrada\n",
    "    Retorna componentes (autovetores), variancias (autovalores), e média.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=None, method='svd', whiten=False):\n",
    "        self.n_components = n_components\n",
    "        self.method = method\n",
    "        self.whiten = bool(whiten)\n",
    "\n",
    "        self.components_ = None        # shape (q, p)\n",
    "        self.explained_variance_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "        self.mean_ = None\n",
    "        self.n_samples_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"X deve ser 2D (n_samples, n_features).\")\n",
    "        n, p = X.shape\n",
    "        self.n_samples_ = n\n",
    "        self.n_features_ = p\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        Xc = X - self.mean_\n",
    "\n",
    "        if self.method == 'eig':\n",
    "            # matriz de covariancia (unbiased)\n",
    "            C = np.dot(Xc.T, Xc) / (n - 1)\n",
    "            vals, vecs = np.linalg.eigh(C)   # eigh para simétrica\n",
    "            # ordenar decrescente\n",
    "            idx = np.argsort(vals)[::-1]\n",
    "            vals = vals[idx]\n",
    "            vecs = vecs[:, idx]\n",
    "        elif self.method == 'svd':\n",
    "            # SVD direto em Xc: Xc = U S Vt\n",
    "            # autovalores de C = (S^2)/(n-1)\n",
    "            U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "            vals = (S**2) / (n - 1)\n",
    "            vecs = Vt.T\n",
    "        else:\n",
    "            raise ValueError(\"method deve ser 'eig' ou 'svd'.\")\n",
    "\n",
    "        # número de componentes\n",
    "        if self.n_components is None:\n",
    "            q = min(n, p)\n",
    "        else:\n",
    "            q = int(self.n_components)\n",
    "            if q <= 0 or q > min(n, p):\n",
    "                raise ValueError(\"n_components inválido.\")\n",
    "\n",
    "        self.components_ = vecs[:, :q].T            # (q, p)\n",
    "        self.explained_variance_ = vals[:q].copy()\n",
    "        total_var = vals.sum()\n",
    "        self.explained_variance_ratio_ = (vals[:q] / total_var) if total_var > 0 else np.zeros_like(vals[:q])\n",
    "\n",
    "        if self.whiten:\n",
    "            # divide componentes por sqrt(autovalores)\n",
    "            eps = 1e-12\n",
    "            self.components_ = self.components_ / np.sqrt(self.explained_variance_[:, None] + eps)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if self.components_ is None:\n",
    "            raise RuntimeError(\"Chame fit(X) antes de transform.\")\n",
    "        Xc = X - self.mean_\n",
    "        return np.dot(Xc, self.components_.T)   # (n_samples, q)\n",
    "\n",
    "    def inverse_transform(self, Z):\n",
    "        Z = np.asarray(Z, dtype=float)\n",
    "        if self.components_ is None:\n",
    "            raise RuntimeError(\"Chame fit(X) antes de inverse_transform.\")\n",
    "        Xc_rec = np.dot(Z, self.components_)\n",
    "        return Xc_rec + self.mean_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cqg = []\n",
    "acc_mdp = []\n",
    "\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    idx = np.random.permutation(X.shape[0])\n",
    "    X, y = X[idx], y[idx]\n",
    "\n",
    "    split = int(0.7 * len(X))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    model_cqg = CQG()\n",
    "    model_mdp = DMP(k_max=6, Nr=11, kmeans_params={'init': 'kmeans++', 'random_state': 42}, validation_indices=['davies'])\n",
    "\n",
    "    model_cqg.fit(X_train, y_train)\n",
    "    y_pred_cqg = model_cqg.predict(X_test)\n",
    "\n",
    "    model_mdp.fit(X_train, y_train)\n",
    "    y_pred_mdp = model_mdp.predict(X_test)\n",
    "\n",
    "    acc_cqg.append(np.mean(y_pred_cqg == y_test.ravel()))\n",
    "    acc_mdp.append(np.mean(y_pred_mdp == y_test.ravel()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "860019f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mdp = DMP(k_max=6, Nr=11, kmeans_params={'init': 'kmeans++'}, validation_indices=['davies'])\n",
    "\n",
    "model_mdp.fit(X, y)\n",
    "y_pred = model_mdp.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407b813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Slight-Right-Turn'],\n",
       "       ['Sharp-Right-Turn'],\n",
       "       ['Sharp-Right-Turn'],\n",
       "       ...,\n",
       "       ['Move-Forward'],\n",
       "       ['Move-Forward'],\n",
       "       ['Move-Forward']], shape=(5456, 1), dtype='<U17')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10a2b1b0",
   "metadata": {},
   "source": [
    "## Questão 02\n",
    "\n",
    "Aplicar PCA ao conjunto original de 24 sensores. Pede-se:\n",
    "\n",
    "2.1. Determinar o número de componenets (q) adequado para o problema, ou seja, que promova uma redução de dimensão dos vetores de atributo sem piorar o desempenho dos classificadores implementados. Mostre o gráfico da variância explicada VE(q).\n",
    "\n",
    "2.2. Repetir o experimento so Subitem 1.3. para os dados transformados por PCA, preenchendo uma tabela de resultados similar. Comente os resultados obtidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21487822",
   "metadata": {},
   "source": [
    "**OBS.: A implementação das tarefas pedidas nas Questões 1 e 2 é feita simultaneamente, na verdade. Enquano um classificador é testado sem PCA, já se pode testa-lo também após aplicação de PCA. Apenas a apresentação dos resultados é que é separada em duas questões para facilitar melhoro entendimento do efeito da aplicação de PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011104e",
   "metadata": {},
   "source": [
    "---\n",
    "¹ Este classificador é construído aplicando-se o algoritmo de K-médias aos dados de cada classe em separado. Cada classe terá seu número de protótipos, sendo que estes herdam o rótulo da classe a qual pertencem. Durante o teste, funciona como classificador distância mínima ao centroide, onde deve-se encontrar o protótipo da classe mais próxima. Para mais detalhes, vide slides do assunto \"Introdução à Clusterização - Métodos Particionais\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reconhecimento-de-padroes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
